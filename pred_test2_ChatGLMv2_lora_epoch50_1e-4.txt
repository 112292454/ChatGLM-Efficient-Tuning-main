nohup: ignoring input
07/25/2023 06:27:09 - WARNING - pet.core.parse - `ddp_find_unused_parameters` needs to be set as False in DDP training.
07/25/2023 06:27:09 - INFO - pet.core.parse - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, 16-bits training: False
07/25/2023 06:27:09 - INFO - pet.core.parse - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4/runs/Jul25_06-27-09_bupt,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
07/25/2023 06:27:09 - INFO - dsets.loader - Loading dataset test2.json...
07/25/2023 06:27:09 - WARNING - dsets.loader - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
07/25/2023 06:27:10 - INFO - datasets.builder - Using custom data configuration default-7ffe8d8bc376eafa
07/25/2023 06:27:10 - INFO - datasets.info - Loading Dataset Infos from /home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/packaged_modules/json
07/25/2023 06:27:10 - INFO - datasets.builder - Generating dataset json (/home/guozeyuan/.cache/huggingface/datasets/json/default-7ffe8d8bc376eafa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/guozeyuan/.cache/huggingface/datasets/json/default-7ffe8d8bc376eafa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 8612.53it/s]
07/25/2023 06:27:10 - INFO - datasets.download.download_manager - Downloading took 0.0 min
07/25/2023 06:27:10 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 160.03it/s]
07/25/2023 06:27:10 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 869 examples [00:00, 7703.03 examples/s]                                                                07/25/2023 06:27:10 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/guozeyuan/.cache/huggingface/datasets/json/default-7ffe8d8bc376eafa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 16.28it/s]
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:27:12,519 >> loading file tokenizer.model from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/tokenizer.model
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:27:12,519 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:27:12,519 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:27:12,519 >> loading file tokenizer_config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/tokenizer_config.json
[INFO|configuration_utils.py:669] 2023-07-25 06:27:12,854 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/config.json
[INFO|configuration_utils.py:669] 2023-07-25 06:27:13,340 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/config.json
[INFO|configuration_utils.py:725] 2023-07-25 06:27:13,342 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm2-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|modeling_utils.py:2578] 2023-07-25 06:27:15,358 >> loading weights file pytorch_model.bin from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/pytorch_model.bin.index.json
Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]Downloading shards:  14%|█▍        | 1/7 [00:00<00:01,  4.05it/s]Downloading shards:  29%|██▊       | 2/7 [00:00<00:01,  3.42it/s]Downloading shards:  43%|████▎     | 3/7 [00:00<00:01,  3.71it/s]Downloading shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.88it/s]Downloading shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.84it/s]Downloading shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.96it/s]Downloading shards: 100%|██████████| 7/7 [00:01<00:00,  4.02it/s]Downloading shards: 100%|██████████| 7/7 [00:01<00:00,  3.90it/s]
[INFO|configuration_utils.py:577] 2023-07-25 06:27:17,179 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:12<01:13, 12.25s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:37<01:38, 19.76s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:53<01:13, 18.26s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [01:20<01:04, 21.51s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [01:34<00:38, 19.01s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [01:55<00:19, 19.67s/it]Loading checkpoint shards: 100%|██████████| 7/7 [02:01<00:00, 15.24s/it]Loading checkpoint shards: 100%|██████████| 7/7 [02:01<00:00, 17.41s/it]
[INFO|modeling_utils.py:3295] 2023-07-25 06:29:19,384 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2023-07-25 06:29:19,385 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm2-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-07-25 06:29:19,648 >> Generation config file not found, using a generation config created from the model config.
07/25/2023 06:29:19 - INFO - pet.core.adapter - Fine-tuning method: LoRA
Traceback (most recent call last):
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 21, in <module>
    main()
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 8, in main
    run_sft(model_args, data_args, training_args, finetuning_args)
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/pet/sft/workflow.py", line 24, in run_sft
    model, tokenizer = load_model_and_tokenizer(model_args, finetuning_args, training_args.do_train, stage="sft")
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/pet/core/model.py", line 145, in load_model_and_tokenizer
    model = init_adapter(model, model_args, finetuning_args, is_trainable)
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/pet/core/adapter.py", line 65, in init_adapter
    assert os.path.exists(os.path.join(model_args.checkpoint_dir[0], WEIGHTS_NAME)), \
AssertionError: Provided path (checkpoint_test2_ChatGLMv2_lora_epoch50_1e-4) does not contain a LoRA weight.
nohup: ignoring input
07/25/2023 06:41:46 - WARNING - pet.core.parse - `ddp_find_unused_parameters` needs to be set as False in DDP training.
07/25/2023 06:41:46 - INFO - pet.core.parse - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, 16-bits training: False
07/25/2023 06:41:46 - INFO - pet.core.parse - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4/runs/Jul25_06-41-46_bupt,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
07/25/2023 06:41:46 - INFO - dsets.loader - Loading dataset test2.json...
07/25/2023 06:41:46 - WARNING - dsets.loader - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
07/25/2023 06:41:47 - INFO - datasets.builder - Using custom data configuration default-7ffe8d8bc376eafa
07/25/2023 06:41:47 - INFO - datasets.info - Loading Dataset Infos from /home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/packaged_modules/json
07/25/2023 06:41:47 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
07/25/2023 06:41:47 - INFO - datasets.info - Loading Dataset info from /home/guozeyuan/.cache/huggingface/datasets/json/default-7ffe8d8bc376eafa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
07/25/2023 06:41:47 - WARNING - datasets.builder - Found cached dataset json (/home/guozeyuan/.cache/huggingface/datasets/json/default-7ffe8d8bc376eafa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
07/25/2023 06:41:47 - INFO - datasets.info - Loading Dataset info from /home/guozeyuan/.cache/huggingface/datasets/json/default-7ffe8d8bc376eafa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 13.93it/s]
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:41:48,800 >> loading file tokenizer.model from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/tokenizer.model
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:41:48,800 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:41:48,800 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:41:48,800 >> loading file tokenizer_config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/tokenizer_config.json
[INFO|configuration_utils.py:669] 2023-07-25 06:41:49,101 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/config.json
[INFO|configuration_utils.py:669] 2023-07-25 06:41:49,590 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/config.json
[INFO|configuration_utils.py:725] 2023-07-25 06:41:49,592 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm2-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|modeling_utils.py:2578] 2023-07-25 06:41:50,296 >> loading weights file pytorch_model.bin from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-07-25 06:41:50,301 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:08<00:48,  8.09s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:19<00:49,  9.88s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:44<01:08, 17.03s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:48<00:35, 11.94s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:51<00:17,  8.64s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:56<00:07,  7.38s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:58<00:00,  5.70s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:58<00:00,  8.41s/it]
[INFO|modeling_utils.py:3295] 2023-07-25 06:42:49,298 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2023-07-25 06:42:49,299 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm2-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-07-25 06:42:49,542 >> Generation config file not found, using a generation config created from the model config.
07/25/2023 06:42:49 - INFO - pet.core.adapter - Fine-tuning method: LoRA
07/25/2023 06:43:13 - INFO - pet.core.adapter - Merged 1 model checkpoint(s).
07/25/2023 06:43:13 - INFO - pet.core.adapter - Loaded fine-tuned model from checkpoint(s): checkpoint_CCKS_ChatGLMv2_lora_epoch50_1e-4
trainable params: 0 || all params: 6243584000 || trainable%: 0.0000
Running tokenizer on dataset:   0%|          | 0/869 [00:00<?, ? examples/s]07/25/2023 06:43:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/guozeyuan/.cache/huggingface/datasets/json/default-7ffe8d8bc376eafa/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9dd0a8583944fd01.arrow
                                                                            Traceback (most recent call last):
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 21, in <module>
    main()
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 8, in main
    run_sft(model_args, data_args, training_args, finetuning_args)
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/pet/sft/workflow.py", line 25, in run_sft
    dataset = preprocess_dataset(dataset, tokenizer, data_args, training_args, stage="sft")
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/dsets/preprocess.py", line 137, in preprocess_dataset
    print_sft_dataset_example(dataset[0])
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2792, in __getitem__
    return self._getitem(key)
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2776, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 583, in query_table
    _check_valid_index_key(key, size)
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 526, in _check_valid_index_key
    raise IndexError(f"Invalid key: {key} is out of bounds for size {size}")
IndexError: Invalid key: 0 is out of bounds for size 0
nohup: ignoring input
07/25/2023 06:55:14 - WARNING - pet.core.parse - `ddp_find_unused_parameters` needs to be set as False in DDP training.
07/25/2023 06:55:14 - INFO - pet.core.parse - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, 16-bits training: False
07/25/2023 06:55:14 - INFO - pet.core.parse - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4/runs/Jul25_06-55-14_bupt,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=evaluation_test2_ChatGLMv2_lora_epoch50_1e-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
07/25/2023 06:55:14 - INFO - dsets.loader - Loading dataset test2.json...
07/25/2023 06:55:14 - WARNING - dsets.loader - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
07/25/2023 06:55:16 - INFO - datasets.builder - Using custom data configuration default-487d7dbd3af699f4
07/25/2023 06:55:16 - INFO - datasets.info - Loading Dataset Infos from /home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/packaged_modules/json
07/25/2023 06:55:16 - INFO - datasets.builder - Generating dataset json (/home/guozeyuan/.cache/huggingface/datasets/json/default-487d7dbd3af699f4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/guozeyuan/.cache/huggingface/datasets/json/default-487d7dbd3af699f4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]
07/25/2023 06:55:16 - INFO - datasets.download.download_manager - Downloading took 0.0 min
07/25/2023 06:55:16 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 36.60it/s]
07/25/2023 06:55:16 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]                                                        07/25/2023 06:55:16 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/guozeyuan/.cache/huggingface/datasets/json/default-487d7dbd3af699f4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 141.11it/s]
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:55:17,225 >> loading file tokenizer.model from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/tokenizer.model
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:55:17,225 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:55:17,225 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-25 06:55:17,225 >> loading file tokenizer_config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/tokenizer_config.json
[INFO|configuration_utils.py:669] 2023-07-25 06:55:17,535 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/config.json
[INFO|configuration_utils.py:669] 2023-07-25 06:55:18,047 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/config.json
[INFO|configuration_utils.py:725] 2023-07-25 06:55:18,048 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm2-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|modeling_utils.py:2578] 2023-07-25 06:55:18,737 >> loading weights file pytorch_model.bin from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/b1502f4f75c71499a3d566b14463edd62620ce9f/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-07-25 06:55:18,752 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:13<01:22, 13.82s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:25<01:03, 12.75s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:27<00:31,  7.90s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:30<00:16,  5.64s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:33<00:09,  4.75s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:35<00:03,  3.97s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:36<00:00,  3.04s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:36<00:00,  5.27s/it]
[INFO|modeling_utils.py:3295] 2023-07-25 06:55:55,736 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2023-07-25 06:55:55,736 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm2-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-07-25 06:55:55,980 >> Generation config file not found, using a generation config created from the model config.
07/25/2023 06:55:55 - INFO - pet.core.adapter - Fine-tuning method: LoRA
07/25/2023 06:56:16 - INFO - pet.core.adapter - Merged 1 model checkpoint(s).
07/25/2023 06:56:16 - INFO - pet.core.adapter - Loaded fine-tuned model from checkpoint(s): checkpoint_CCKS_ChatGLMv2_lora_epoch50_1e-4
trainable params: 0 || all params: 6243584000 || trainable%: 0.0000
Running tokenizer on dataset:   0%|          | 0/869 [00:00<?, ? examples/s]07/25/2023 06:56:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/guozeyuan/.cache/huggingface/datasets/json/default-487d7dbd3af699f4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9555afc9cceb1f34.arrow
Running tokenizer on dataset: 100%|██████████| 869/869 [00:00<00:00, 3293.38 examples/s]                                                                                        input_ids:
[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 36454, 31623, 6303, 1568, 8539, 30910, 34262, 31123, 45843, 54619, 32040, 54840, 54621, 31639, 35199, 35794, 31211, 55910, 54539, 33761, 32944, 54532, 34711, 54556, 54666, 30987, 13, 13, 55437, 31211]
inputs:
[Round 1]

问：生成一个 SPARQL 查询，检索与以下给定问题对应的信息：浙大现在的校长是哪个来着?

答：
label_ids:
[64790, 64792, 260, 24846]
labels:
aaaaa
[INFO|trainer.py:3200] 2023-07-25 06:56:28,119 >> ***** Running Prediction *****
[INFO|trainer.py:3202] 2023-07-25 06:56:28,120 >>   Num examples = 869
[INFO|trainer.py:3205] 2023-07-25 06:56:28,120 >>   Batch size = 8
[INFO|configuration_utils.py:577] 2023-07-25 06:56:28,306 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

  0%|          | 0/109 [00:00<?, ?it/s]  2%|▏         | 2/109 [00:01<01:29,  1.19it/s]  3%|▎         | 3/109 [00:02<01:49,  1.03s/it]  4%|▎         | 4/109 [00:04<02:08,  1.22s/it]  5%|▍         | 5/109 [00:05<02:08,  1.24s/it]  6%|▌         | 6/109 [00:07<02:22,  1.38s/it]  6%|▋         | 7/109 [00:08<02:23,  1.41s/it]  7%|▋         | 8/109 [00:11<02:53,  1.71s/it]  8%|▊         | 9/109 [00:12<02:48,  1.68s/it]  9%|▉         | 10/109 [00:14<02:39,  1.61s/it] 10%|█         | 11/109 [00:15<02:31,  1.54s/it] 11%|█         | 12/109 [00:17<02:23,  1.48s/it] 12%|█▏        | 13/109 [00:19<02:45,  1.72s/it] 13%|█▎        | 14/109 [00:20<02:36,  1.64s/it] 14%|█▍        | 15/109 [00:22<02:29,  1.59s/it] 15%|█▍        | 16/109 [00:24<02:45,  1.78s/it] 16%|█▌        | 17/109 [00:26<02:56,  1.91s/it] 17%|█▋        | 18/109 [00:28<02:42,  1.78s/it] 17%|█▋        | 19/109 [00:29<02:25,  1.61s/it] 18%|█▊        | 20/109 [00:31<02:27,  1.66s/it] 19%|█▉        | 21/109 [00:32<02:14,  1.53s/it] 20%|██        | 22/109 [00:34<02:20,  1.62s/it] 21%|██        | 23/109 [00:36<02:40,  1.87s/it] 22%|██▏       | 24/109 [00:39<02:54,  2.05s/it] 23%|██▎       | 25/109 [00:41<02:52,  2.05s/it] 24%|██▍       | 26/109 [00:42<02:39,  1.93s/it] 25%|██▍       | 27/109 [00:44<02:28,  1.81s/it] 26%|██▌       | 28/109 [00:46<02:28,  1.84s/it] 27%|██▋       | 29/109 [00:47<02:08,  1.61s/it] 28%|██▊       | 30/109 [00:48<01:56,  1.48s/it] 28%|██▊       | 31/109 [00:51<02:23,  1.84s/it] 29%|██▉       | 32/109 [00:53<02:23,  1.86s/it] 30%|███       | 33/109 [00:54<02:16,  1.79s/it] 31%|███       | 34/109 [00:55<02:00,  1.61s/it] 32%|███▏      | 35/109 [00:57<02:06,  1.71s/it] 33%|███▎      | 36/109 [01:01<02:35,  2.13s/it] 34%|███▍      | 37/109 [01:03<02:33,  2.13s/it] 35%|███▍      | 38/109 [01:04<02:20,  1.98s/it] 36%|███▌      | 39/109 [01:06<02:08,  1.83s/it] 37%|███▋      | 40/109 [01:07<01:55,  1.68s/it] 38%|███▊      | 41/109 [01:09<01:55,  1.70s/it] 39%|███▊      | 42/109 [01:11<01:54,  1.70s/it] 39%|███▉      | 43/109 [01:12<01:52,  1.71s/it] 40%|████      | 44/109 [01:13<01:41,  1.56s/it] 41%|████▏     | 45/109 [01:15<01:34,  1.48s/it] 42%|████▏     | 46/109 [01:17<01:39,  1.58s/it] 43%|████▎     | 47/109 [01:18<01:41,  1.63s/it] 44%|████▍     | 48/109 [01:20<01:43,  1.70s/it] 45%|████▍     | 49/109 [01:22<01:35,  1.59s/it] 46%|████▌     | 50/109 [01:23<01:37,  1.66s/it] 47%|████▋     | 51/109 [01:26<01:46,  1.83s/it] 48%|████▊     | 52/109 [01:27<01:44,  1.83s/it] 49%|████▊     | 53/109 [01:30<01:47,  1.92s/it] 50%|████▉     | 54/109 [01:31<01:39,  1.81s/it] 50%|█████     | 55/109 [01:33<01:33,  1.74s/it] 51%|█████▏    | 56/109 [01:34<01:32,  1.75s/it] 52%|█████▏    | 57/109 [01:37<01:42,  1.97s/it] 53%|█████▎    | 58/109 [01:40<01:49,  2.15s/it] 54%|█████▍    | 59/109 [01:41<01:36,  1.93s/it] 55%|█████▌    | 60/109 [01:42<01:26,  1.76s/it] 56%|█████▌    | 61/109 [01:44<01:20,  1.67s/it] 57%|█████▋    | 62/109 [01:46<01:20,  1.71s/it] 58%|█████▊    | 63/109 [01:48<01:27,  1.91s/it] 59%|█████▊    | 64/109 [01:50<01:26,  1.92s/it] 60%|█████▉    | 65/109 [01:52<01:29,  2.04s/it] 61%|██████    | 66/109 [01:54<01:22,  1.92s/it] 61%|██████▏   | 67/109 [01:56<01:21,  1.95s/it] 62%|██████▏   | 68/109 [01:57<01:15,  1.84s/it] 63%|██████▎   | 69/109 [01:59<01:15,  1.88s/it] 64%|██████▍   | 70/109 [02:02<01:21,  2.10s/it] 65%|██████▌   | 71/109 [02:04<01:22,  2.17s/it] 66%|██████▌   | 72/109 [02:06<01:13,  2.00s/it] 67%|██████▋   | 73/109 [02:08<01:09,  1.93s/it] 68%|██████▊   | 74/109 [02:09<01:04,  1.85s/it] 69%|██████▉   | 75/109 [02:11<01:03,  1.86s/it] 70%|██████▉   | 76/109 [02:13<01:02,  1.90s/it] 71%|███████   | 77/109 [02:15<01:01,  1.92s/it] 72%|███████▏  | 78/109 [02:17<00:57,  1.87s/it] 72%|███████▏  | 79/109 [02:19<00:58,  1.96s/it] 73%|███████▎  | 80/109 [02:21<00:59,  2.06s/it] 74%|███████▍  | 81/109 [02:23<00:52,  1.89s/it] 75%|███████▌  | 82/109 [02:25<00:55,  2.07s/it] 76%|███████▌  | 83/109 [02:27<00:47,  1.84s/it] 77%|███████▋  | 84/109 [02:28<00:41,  1.66s/it] 78%|███████▊  | 85/109 [02:30<00:39,  1.63s/it] 79%|███████▉  | 86/109 [02:31<00:39,  1.72s/it] 80%|███████▉  | 87/109 [02:33<00:39,  1.81s/it] 81%|████████  | 88/109 [02:37<00:46,  2.19s/it] 82%|████████▏ | 89/109 [02:39<00:47,  2.36s/it] 83%|████████▎ | 90/109 [02:43<00:50,  2.64s/it] 83%|████████▎ | 91/109 [02:45<00:46,  2.60s/it] 84%|████████▍ | 92/109 [02:47<00:39,  2.31s/it] 85%|████████▌ | 93/109 [02:49<00:38,  2.39s/it] 86%|████████▌ | 94/109 [02:53<00:40,  2.71s/it] 87%|████████▋ | 95/109 [02:55<00:35,  2.51s/it] 88%|████████▊ | 96/109 [02:57<00:32,  2.49s/it] 89%|████████▉ | 97/109 [03:00<00:31,  2.60s/it] 90%|████████▉ | 98/109 [03:03<00:30,  2.73s/it] 91%|█████████ | 99/109 [03:06<00:27,  2.79s/it] 92%|█████████▏| 100/109 [03:09<00:25,  2.84s/it] 93%|█████████▎| 101/109 [03:12<00:22,  2.86s/it] 94%|█████████▎| 102/109 [03:15<00:21,  3.01s/it] 94%|█████████▍| 103/109 [03:19<00:18,  3.09s/it] 95%|█████████▌| 104/109 [03:23<00:16,  3.36s/it] 96%|█████████▋| 105/109 [03:25<00:12,  3.18s/it] 97%|█████████▋| 106/109 [03:29<00:10,  3.40s/it] 98%|█████████▊| 107/109 [03:31<00:05,  2.91s/it] 99%|█████████▉| 108/109 [03:33<00:02,  2.69s/it]100%|██████████| 109/109 [03:35<00:00,  2.50s/it]Building prefix dict from the default dictionary ...
07/25/2023 07:00:13 - DEBUG - jieba - Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
07/25/2023 07:00:13 - DEBUG - jieba - Dumping model to file cache /tmp/jieba.cache
Loading model cost 0.697 seconds.
07/25/2023 07:00:13 - DEBUG - jieba - Loading model cost 0.697 seconds.
Prefix dict has been built successfully.
07/25/2023 07:00:13 - DEBUG - jieba - Prefix dict has been built successfully.
100%|██████████| 109/109 [03:37<00:00,  2.00s/it]
***** predict metrics *****
  predict_bleu-4             =     0.0217
  predict_rouge-1            =        0.0
  predict_rouge-2            =        0.0
  predict_rouge-l            =        0.0
  predict_runtime            = 0:03:46.00
  predict_samples_per_second =      3.845
  predict_steps_per_second   =      0.482
07/25/2023 07:00:14 - INFO - pet.sft.trainer - Saving prediction results to evaluation_test2_ChatGLMv2_lora_epoch50_1e-4/generated_predictions.jsonl

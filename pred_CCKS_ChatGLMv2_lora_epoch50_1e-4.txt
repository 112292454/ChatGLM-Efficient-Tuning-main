nohup: ignoring input
Traceback (most recent call last):
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 21, in <module>
    main()
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 6, in main
    model_args, data_args, training_args, finetuning_args, general_args = get_train_args()
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/pet/core/parse.py", line 49, in get_train_args
    data_args.init_for_training()
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/hparams/data_args.py", line 86, in init_for_training
    raise ValueError("Undefined dataset {} in dataset_info.json.".format(name))
ValueError: Undefined dataset CCKS_test in dataset_info.json.
nohup: ignoring input
07/19/2023 03:09:55 - WARNING - pet.core.parse - `ddp_find_unused_parameters` needs to be set as False in DDP training.
07/19/2023 03:09:55 - INFO - pet.core.parse - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, 16-bits training: False
07/19/2023 03:09:55 - INFO - pet.core.parse - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=evaluation_CCKS_ChatGLMv2_lora_epoch50_1e-4/runs/Jul19_03-09-55_bupt,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=evaluation_CCKS_ChatGLMv2_lora_epoch50_1e-4,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=evaluation_CCKS_ChatGLMv2_lora_epoch50_1e-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
07/19/2023 03:09:55 - INFO - dsets.loader - Loading dataset CCKS_test.json...
07/19/2023 03:09:55 - WARNING - dsets.loader - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
07/19/2023 03:09:56 - INFO - datasets.builder - Using custom data configuration default-9990722a6f25825f
07/19/2023 03:09:56 - INFO - datasets.info - Loading Dataset Infos from /home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/packaged_modules/json
07/19/2023 03:09:56 - INFO - datasets.builder - Generating dataset json (/home/guozeyuan/.cache/huggingface/datasets/json/default-9990722a6f25825f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/guozeyuan/.cache/huggingface/datasets/json/default-9990722a6f25825f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]
07/19/2023 03:09:56 - INFO - datasets.download.download_manager - Downloading took 0.0 min
07/19/2023 03:09:56 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 908.84it/s]
07/19/2023 03:09:56 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]                                                        07/19/2023 03:09:56 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/guozeyuan/.cache/huggingface/datasets/json/default-9990722a6f25825f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 677.48it/s]
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:09:57,206 >> loading file tokenizer.model from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/tokenizer.model
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:09:57,206 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:09:57,206 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:09:57,206 >> loading file tokenizer_config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/tokenizer_config.json
[INFO|configuration_utils.py:669] 2023-07-19 03:09:57,513 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/config.json
[INFO|configuration_utils.py:669] 2023-07-19 03:09:58,017 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/config.json
[INFO|configuration_utils.py:725] 2023-07-19 03:09:58,019 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm2-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|modeling_utils.py:2578] 2023-07-19 03:09:58,702 >> loading weights file pytorch_model.bin from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-07-19 03:09:58,717 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:07<00:43,  7.27s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:15<00:38,  7.64s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:22<00:30,  7.63s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:30<00:22,  7.55s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:38<00:15,  7.73s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:42<00:06,  6.47s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:44<00:00,  5.15s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:44<00:00,  6.39s/it]
[INFO|modeling_utils.py:3295] 2023-07-19 03:10:43,580 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2023-07-19 03:10:43,580 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm2-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-07-19 03:10:43,830 >> Generation config file not found, using a generation config created from the model config.
07/19/2023 03:10:43 - INFO - pet.core.adapter - Fine-tuning method: LoRA
07/19/2023 03:10:59 - INFO - pet.core.adapter - Merged 1 model checkpoint(s).
07/19/2023 03:10:59 - INFO - pet.core.adapter - Loaded fine-tuned model from checkpoint(s): checkpoint_CCKS_ChatGLMv2_lora_epoch50_1e-4
trainable params: 0 || all params: 6243584000 || trainable%: 0.0000
Running tokenizer on dataset:   0%|          | 0/1292 [00:00<?, ? examples/s]07/19/2023 03:10:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/guozeyuan/.cache/huggingface/datasets/json/default-9990722a6f25825f/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-60f72e3afa49f6a0.arrow
                                                                             Traceback (most recent call last):
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 21, in <module>
    main()
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/train_bash.py", line 8, in main
    run_sft(model_args, data_args, training_args, finetuning_args)
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/pet/sft/workflow.py", line 25, in run_sft
    dataset = preprocess_dataset(dataset, tokenizer, data_args, training_args, stage="sft")
  File "/home/guozeyuan/reduce/ChatGLM-Efficient-Tuning/src/dsets/preprocess.py", line 137, in preprocess_dataset
    print_sft_dataset_example(dataset[0])
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2792, in __getitem__
    return self._getitem(key)
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/arrow_dataset.py", line 2776, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 583, in query_table
    _check_valid_index_key(key, size)
  File "/home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/formatting/formatting.py", line 526, in _check_valid_index_key
    raise IndexError(f"Invalid key: {key} is out of bounds for size {size}")
IndexError: Invalid key: 0 is out of bounds for size 0
nohup: ignoring input
07/19/2023 03:13:22 - WARNING - pet.core.parse - `ddp_find_unused_parameters` needs to be set as False in DDP training.
07/19/2023 03:13:22 - INFO - pet.core.parse - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, 16-bits training: False
07/19/2023 03:13:22 - INFO - pet.core.parse - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=True,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=evaluation_CCKS_ChatGLMv2_lora_epoch50_1e-4/runs/Jul19_03-13-22_bupt,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
output_dir=evaluation_CCKS_ChatGLMv2_lora_epoch50_1e-4,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=evaluation_CCKS_ChatGLMv2_lora_epoch50_1e-4,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
07/19/2023 03:13:22 - INFO - dsets.loader - Loading dataset CCKS_test.json...
07/19/2023 03:13:22 - WARNING - dsets.loader - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.
07/19/2023 03:13:23 - INFO - datasets.builder - Using custom data configuration default-d58feff77f1ab45b
07/19/2023 03:13:23 - INFO - datasets.info - Loading Dataset Infos from /home/guozeyuan/miniconda3/envs/eglm/lib/python3.10/site-packages/datasets/packaged_modules/json
07/19/2023 03:13:23 - INFO - datasets.builder - Generating dataset json (/home/guozeyuan/.cache/huggingface/datasets/json/default-d58feff77f1ab45b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/guozeyuan/.cache/huggingface/datasets/json/default-d58feff77f1ab45b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 5890.88it/s]
07/19/2023 03:13:23 - INFO - datasets.download.download_manager - Downloading took 0.0 min
07/19/2023 03:13:23 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 843.92it/s]
07/19/2023 03:13:23 - INFO - datasets.builder - Generating train split
Generating train split: 0 examples [00:00, ? examples/s]                                                        07/19/2023 03:13:23 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/guozeyuan/.cache/huggingface/datasets/json/default-d58feff77f1ab45b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 469.90it/s]
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:13:24,071 >> loading file tokenizer.model from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/tokenizer.model
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:13:24,071 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:13:24,071 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1823] 2023-07-19 03:13:24,071 >> loading file tokenizer_config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/tokenizer_config.json
[INFO|configuration_utils.py:669] 2023-07-19 03:13:24,350 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/config.json
[INFO|configuration_utils.py:669] 2023-07-19 03:13:25,245 >> loading configuration file config.json from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/config.json
[INFO|configuration_utils.py:725] 2023-07-19 03:13:25,247 >> Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm2-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm2-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm2-6b--modeling_chatglm.ChatGLMForConditionalGeneration"
  },
  "bias_dropout_fusion": true,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|modeling_utils.py:2578] 2023-07-19 03:13:25,607 >> loading weights file pytorch_model.bin from cache at /home/guozeyuan/.cache/huggingface/hub/models--THUDM--chatglm2-6b/snapshots/4e38bef4c028beafc8fb1837462f74c02e68fcc2/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2023-07-19 03:13:25,609 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:05<00:30,  5.02s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:06<00:15,  3.01s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:08<00:09,  2.33s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:09<00:06,  2.05s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:11<00:04,  2.07s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:16<00:02,  2.83s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:17<00:00,  2.19s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:17<00:00,  2.44s/it]
[INFO|modeling_utils.py:3295] 2023-07-19 03:13:42,783 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:3303] 2023-07-19 03:13:42,784 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at THUDM/chatglm2-6b.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|modeling_utils.py:2927] 2023-07-19 03:13:43,031 >> Generation config file not found, using a generation config created from the model config.
07/19/2023 03:13:43 - INFO - pet.core.adapter - Fine-tuning method: LoRA
07/19/2023 03:13:56 - INFO - pet.core.adapter - Merged 1 model checkpoint(s).
07/19/2023 03:13:56 - INFO - pet.core.adapter - Loaded fine-tuned model from checkpoint(s): checkpoint_CCKS_ChatGLMv2_lora_epoch50_1e-4
trainable params: 0 || all params: 6243584000 || trainable%: 0.0000
Running tokenizer on dataset:   0%|          | 0/1292 [00:00<?, ? examples/s]07/19/2023 03:13:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/guozeyuan/.cache/huggingface/datasets/json/default-d58feff77f1ab45b/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-b4de50307e8d491e.arrow
Running tokenizer on dataset:  77%|███████▋  | 1000/1292 [00:00<00:00, 4522.24 examples/s]                                                                                          input_ids:
[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 36454, 31623, 6303, 1568, 8539, 30910, 34262, 31123, 45843, 54619, 32040, 54840, 54621, 31639, 35199, 35794, 31211, 56165, 55681, 54917, 54576, 42217, 37633, 31514, 13, 13, 55437, 31211]
inputs:
[Round 1]

问：生成一个 SPARQL 查询，检索与以下给定问题对应的信息：凉宫村日所在的社团？

答：
label_ids:
[64790, 64792, 1429]
labels:
test
[INFO|trainer.py:3200] 2023-07-19 03:14:03,774 >> ***** Running Prediction *****
[INFO|trainer.py:3202] 2023-07-19 03:14:03,774 >>   Num examples = 1292
[INFO|trainer.py:3205] 2023-07-19 03:14:03,774 >>   Batch size = 8
[INFO|configuration_utils.py:577] 2023-07-19 03:14:03,781 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

  0%|          | 0/162 [00:00<?, ?it/s]  1%|          | 2/162 [00:01<01:45,  1.51it/s]  2%|▏         | 3/162 [00:02<02:23,  1.10it/s]  2%|▏         | 4/162 [00:03<02:50,  1.08s/it]  3%|▎         | 5/162 [00:05<03:27,  1.32s/it]  4%|▎         | 6/162 [00:07<03:50,  1.48s/it]  4%|▍         | 7/162 [00:08<03:34,  1.39s/it]  5%|▍         | 8/162 [00:09<03:14,  1.26s/it]  6%|▌         | 9/162 [00:11<03:21,  1.31s/it]  6%|▌         | 10/162 [00:12<03:25,  1.35s/it]  7%|▋         | 11/162 [00:14<03:59,  1.59s/it]  7%|▋         | 12/162 [00:16<03:57,  1.58s/it]  8%|▊         | 13/162 [00:18<04:19,  1.74s/it]  9%|▊         | 14/162 [00:19<04:01,  1.63s/it]  9%|▉         | 15/162 [00:21<03:57,  1.61s/it] 10%|▉         | 16/162 [00:22<03:26,  1.42s/it] 10%|█         | 17/162 [00:23<03:18,  1.37s/it] 11%|█         | 18/162 [00:25<03:29,  1.45s/it] 12%|█▏        | 19/162 [00:27<04:08,  1.74s/it] 12%|█▏        | 20/162 [00:28<03:47,  1.60s/it] 13%|█▎        | 21/162 [00:30<03:37,  1.54s/it] 14%|█▎        | 22/162 [00:31<03:43,  1.59s/it] 14%|█▍        | 23/162 [00:33<03:36,  1.56s/it] 15%|█▍        | 24/162 [00:34<03:21,  1.46s/it] 15%|█▌        | 25/162 [00:35<03:01,  1.33s/it] 16%|█▌        | 26/162 [00:37<03:11,  1.41s/it] 17%|█▋        | 27/162 [00:38<03:10,  1.41s/it] 17%|█▋        | 28/162 [00:39<03:00,  1.35s/it] 18%|█▊        | 29/162 [00:41<03:14,  1.46s/it] 19%|█▊        | 30/162 [00:43<03:12,  1.46s/it] 19%|█▉        | 31/162 [00:44<03:05,  1.41s/it] 20%|█▉        | 32/162 [00:45<03:02,  1.40s/it] 20%|██        | 33/162 [00:47<02:56,  1.37s/it] 21%|██        | 34/162 [00:48<02:42,  1.27s/it] 22%|██▏       | 35/162 [00:49<02:39,  1.26s/it] 22%|██▏       | 36/162 [00:50<02:47,  1.33s/it] 23%|██▎       | 37/162 [00:52<02:49,  1.35s/it] 23%|██▎       | 38/162 [00:53<03:01,  1.47s/it] 24%|██▍       | 39/162 [00:55<03:00,  1.47s/it] 25%|██▍       | 40/162 [00:56<02:57,  1.46s/it] 25%|██▌       | 41/162 [00:58<03:07,  1.55s/it] 26%|██▌       | 42/162 [01:00<03:00,  1.50s/it] 27%|██▋       | 43/162 [01:01<02:51,  1.44s/it] 27%|██▋       | 44/162 [01:02<02:54,  1.48s/it] 28%|██▊       | 45/162 [01:03<02:37,  1.34s/it] 28%|██▊       | 46/162 [01:05<02:30,  1.30s/it] 29%|██▉       | 47/162 [01:06<02:39,  1.39s/it] 30%|██▉       | 48/162 [01:07<02:27,  1.29s/it] 30%|███       | 49/162 [01:09<02:25,  1.29s/it] 31%|███       | 50/162 [01:10<02:30,  1.34s/it] 31%|███▏      | 51/162 [01:11<02:23,  1.29s/it] 32%|███▏      | 52/162 [01:13<02:33,  1.39s/it] 33%|███▎      | 53/162 [01:14<02:24,  1.33s/it] 33%|███▎      | 54/162 [01:15<02:25,  1.35s/it] 34%|███▍      | 55/162 [01:17<02:18,  1.29s/it] 35%|███▍      | 56/162 [01:18<02:08,  1.22s/it] 35%|███▌      | 57/162 [01:19<02:12,  1.26s/it] 36%|███▌      | 58/162 [01:20<02:16,  1.32s/it] 36%|███▋      | 59/162 [01:22<02:18,  1.35s/it] 37%|███▋      | 60/162 [01:23<02:16,  1.34s/it] 38%|███▊      | 61/162 [01:25<02:17,  1.36s/it] 38%|███▊      | 62/162 [01:26<02:11,  1.32s/it] 39%|███▉      | 63/162 [01:28<02:33,  1.55s/it] 40%|███▉      | 64/162 [01:30<02:39,  1.63s/it] 40%|████      | 65/162 [01:32<02:52,  1.78s/it] 41%|████      | 66/162 [01:33<02:39,  1.66s/it] 41%|████▏     | 67/162 [01:35<02:29,  1.58s/it] 42%|████▏     | 68/162 [01:37<02:45,  1.76s/it] 43%|████▎     | 69/162 [01:39<02:43,  1.76s/it] 43%|████▎     | 70/162 [01:41<03:08,  2.05s/it] 44%|████▍     | 71/162 [01:43<02:47,  1.84s/it] 44%|████▍     | 72/162 [01:44<02:29,  1.66s/it] 45%|████▌     | 73/162 [01:45<02:02,  1.38s/it] 46%|████▌     | 74/162 [01:46<01:59,  1.36s/it] 46%|████▋     | 75/162 [01:47<01:44,  1.20s/it] 47%|████▋     | 76/162 [01:48<01:46,  1.24s/it] 48%|████▊     | 77/162 [01:50<02:05,  1.47s/it] 48%|████▊     | 78/162 [01:51<01:56,  1.38s/it] 49%|████▉     | 79/162 [01:53<02:02,  1.48s/it] 49%|████▉     | 80/162 [01:54<01:51,  1.36s/it] 50%|█████     | 81/162 [01:55<01:49,  1.36s/it] 51%|█████     | 82/162 [01:57<01:47,  1.34s/it] 51%|█████     | 83/162 [01:58<01:47,  1.36s/it] 52%|█████▏    | 84/162 [02:00<01:54,  1.47s/it] 52%|█████▏    | 85/162 [02:02<02:00,  1.56s/it] 53%|█████▎    | 86/162 [02:02<01:43,  1.36s/it] 54%|█████▎    | 87/162 [02:04<01:37,  1.30s/it] 54%|█████▍    | 88/162 [02:05<01:26,  1.17s/it] 55%|█████▍    | 89/162 [02:06<01:36,  1.33s/it] 56%|█████▌    | 90/162 [02:08<01:45,  1.47s/it] 56%|█████▌    | 91/162 [02:11<02:13,  1.87s/it] 57%|█████▋    | 92/162 [02:14<02:35,  2.22s/it] 57%|█████▋    | 93/162 [02:15<02:10,  1.89s/it] 58%|█████▊    | 94/162 [02:16<01:59,  1.76s/it] 59%|█████▊    | 95/162 [02:18<01:48,  1.62s/it] 59%|█████▉    | 96/162 [02:19<01:30,  1.37s/it] 60%|█████▉    | 97/162 [02:19<01:20,  1.24s/it] 60%|██████    | 98/162 [02:21<01:26,  1.34s/it] 61%|██████    | 99/162 [02:23<01:28,  1.41s/it] 62%|██████▏   | 100/162 [02:24<01:23,  1.35s/it] 62%|██████▏   | 101/162 [02:25<01:21,  1.34s/it] 63%|██████▎   | 102/162 [02:26<01:20,  1.35s/it] 64%|██████▎   | 103/162 [02:28<01:15,  1.28s/it] 64%|██████▍   | 104/162 [02:29<01:13,  1.27s/it] 65%|██████▍   | 105/162 [02:30<01:15,  1.33s/it] 65%|██████▌   | 106/162 [02:32<01:16,  1.36s/it] 66%|██████▌   | 107/162 [02:33<01:20,  1.47s/it] 67%|██████▋   | 108/162 [02:35<01:25,  1.58s/it] 67%|██████▋   | 109/162 [02:37<01:21,  1.55s/it] 68%|██████▊   | 110/162 [02:38<01:17,  1.49s/it] 69%|██████▊   | 111/162 [02:39<01:12,  1.42s/it] 69%|██████▉   | 112/162 [02:41<01:15,  1.51s/it] 70%|██████▉   | 113/162 [02:43<01:21,  1.65s/it] 70%|███████   | 114/162 [02:45<01:25,  1.78s/it] 71%|███████   | 115/162 [02:47<01:28,  1.89s/it] 72%|███████▏  | 116/162 [02:49<01:29,  1.95s/it] 72%|███████▏  | 117/162 [02:51<01:24,  1.87s/it] 73%|███████▎  | 118/162 [02:53<01:19,  1.80s/it] 73%|███████▎  | 119/162 [02:54<01:15,  1.75s/it] 74%|███████▍  | 120/162 [02:56<01:10,  1.69s/it] 75%|███████▍  | 121/162 [02:58<01:09,  1.70s/it] 75%|███████▌  | 122/162 [03:00<01:11,  1.79s/it] 76%|███████▌  | 123/162 [03:01<01:08,  1.75s/it] 77%|███████▋  | 124/162 [03:03<01:04,  1.70s/it] 77%|███████▋  | 125/162 [03:04<00:52,  1.42s/it] 78%|███████▊  | 126/162 [03:04<00:44,  1.24s/it] 78%|███████▊  | 127/162 [03:05<00:38,  1.10s/it] 79%|███████▉  | 128/162 [03:06<00:33,  1.01it/s] 80%|███████▉  | 129/162 [03:07<00:30,  1.09it/s] 80%|████████  | 130/162 [03:07<00:27,  1.15it/s] 81%|████████  | 131/162 [03:08<00:25,  1.21it/s] 81%|████████▏ | 132/162 [03:09<00:24,  1.24it/s] 82%|████████▏ | 133/162 [03:10<00:23,  1.26it/s] 83%|████████▎ | 134/162 [03:10<00:21,  1.28it/s] 83%|████████▎ | 135/162 [03:11<00:22,  1.23it/s] 84%|████████▍ | 136/162 [03:12<00:20,  1.24it/s] 85%|████████▍ | 137/162 [03:13<00:19,  1.29it/s] 85%|████████▌ | 138/162 [03:14<00:18,  1.29it/s] 86%|████████▌ | 139/162 [03:14<00:17,  1.28it/s] 86%|████████▋ | 140/162 [03:15<00:16,  1.30it/s] 87%|████████▋ | 141/162 [03:16<00:16,  1.31it/s] 88%|████████▊ | 142/162 [03:17<00:14,  1.34it/s] 88%|████████▊ | 143/162 [03:19<00:23,  1.23s/it] 89%|████████▉ | 144/162 [03:21<00:25,  1.42s/it] 90%|████████▉ | 145/162 [03:23<00:26,  1.55s/it] 90%|█████████ | 146/162 [03:25<00:30,  1.88s/it] 91%|█████████ | 147/162 [03:27<00:29,  1.94s/it] 91%|█████████▏| 148/162 [03:29<00:24,  1.77s/it] 92%|█████████▏| 149/162 [03:31<00:24,  1.88s/it] 93%|█████████▎| 150/162 [03:34<00:25,  2.14s/it] 93%|█████████▎| 151/162 [03:35<00:22,  2.00s/it] 94%|█████████▍| 152/162 [03:37<00:19,  1.97s/it] 94%|█████████▍| 153/162 [03:40<00:18,  2.09s/it] 95%|█████████▌| 154/162 [03:42<00:17,  2.21s/it] 96%|█████████▌| 155/162 [03:45<00:16,  2.31s/it] 96%|█████████▋| 156/162 [03:47<00:13,  2.22s/it] 97%|█████████▋| 157/162 [03:49<00:11,  2.25s/it] 98%|█████████▊| 158/162 [03:52<00:09,  2.39s/it] 98%|█████████▊| 159/162 [03:54<00:07,  2.45s/it] 99%|█████████▉| 160/162 [03:58<00:05,  2.69s/it] 99%|█████████▉| 161/162 [04:01<00:02,  2.98s/it]100%|██████████| 162/162 [04:03<00:00,  2.73s/it]Building prefix dict from the default dictionary ...
07/19/2023 03:18:15 - DEBUG - jieba - Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
07/19/2023 03:18:16 - DEBUG - jieba - Dumping model to file cache /tmp/jieba.cache
Loading model cost 0.654 seconds.
07/19/2023 03:18:16 - DEBUG - jieba - Loading model cost 0.654 seconds.
Prefix dict has been built successfully.
07/19/2023 03:18:16 - DEBUG - jieba - Prefix dict has been built successfully.
100%|██████████| 162/162 [04:06<00:00,  1.52s/it]
***** predict metrics *****
  predict_bleu-4             =     1.0064
  predict_rouge-1            =        0.0
  predict_rouge-2            =        0.0
  predict_rouge-l            =        0.0
  predict_runtime            = 0:04:13.37
  predict_samples_per_second =      5.099
  predict_steps_per_second   =      0.639
07/19/2023 03:18:17 - INFO - pet.sft.trainer - Saving prediction results to evaluation_CCKS_ChatGLMv2_lora_epoch50_1e-4/generated_predictions.jsonl
